# -*- coding: utf-8 -*-
"""ds555.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v1HBmISn_3gc-swAVza78pIFSK7ScjuD

# **Libraries**
"""

import pandas as pd
import numpy as np
from copy import deepcopy as dc
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from sklearn import metrics
from sklearn.linear_model import Lasso, LassoCV
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Ridge 
from sklearn.linear_model import RidgeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn import linear_model
from scipy.stats import zscore
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

"""# **Import data**"""

from google.colab import drive
drive.mount('/content/drive/')

"""# **Read Data & Clean irrelevent Data**"""

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/vehicles.csv')
raw_df = dc(df)
df= df.drop('region_url', axis = 1)
df= df.drop('url', axis = 1)
df= df.drop('id', axis = 1)
df= df.drop('Unnamed: 0', axis = 1)
df= df.drop('image_url', axis = 1)
df= df.drop('description', axis = 1)
df= df[df['year'] > 1980.0]
df= df.drop('drive', axis = 1)
df= df.drop('size', axis = 1)
df= df.drop('posting_date', axis = 1)

"""**Backup data**"""

data = dc(df)
data = data.reset_index(drop=True)
data

fig, ax = plt.subplots()
#data['price'].value_counts().plot(ax=ax, kind='line')
#data['price'].plot(kind='bar')
tempp = data['price'].value_counts(sort=False)#.plot.line()

# df.sort_values(by=['col1'])
# tempp
tempp = tempp.to_frame()
tempp.columns = ['Frequency of price']
tempp = tempp.sort_index()
tempp.plot.line()

data[(data['price'] == 0)]
data = data[(data['price'] <= 98)]
data = data[(data['price'] >= 2)]

"""# **Decoding VIN into Years**"""

data = data.dropna()
data = data.drop_duplicates()
data

year = {'A':[1980.0,2010.0],'B': [1981.0,2011.0],'C':[1982.0,2012.0],'D':[1983.0,2013.0],'E':[1984.0,2014.0],'F':[1985.0,2015.0],'G':[1986.0,2016.0],'H':[1987.0,2017.0],'J':[1988.0,2018.0],'K':[1989.0,2019.0],
        'L':[1990.0,2020.0],'M':[1991.0,2021],'N':[1992.0],'P':[1993.0],'R':[1994.0],'S':[1995.0],'T':[1996.0],'V':[1997.0],'W':[1998.0],'X':[1999.0],
        'Y':[2000.0],'1':[2001.0],'2':[2002.0],'3':[2003.0],'4':[2004.0],'5':[2005.0],'6':[2006.0],'7':[2007.0],'8':[2008.0],'9':[2009.0]}

counter = 0
for i in range(len(data['VIN'])):
  if pd.isnull(data['VIN'][i]) or len(data['VIN'][i])<17 :
    continue
  elif data['VIN'][i][9] not in year:
    continue
  elif data['year'][i] in year[data['VIN'][i][9]]:
    continue
  else:
    if len(year[data['VIN'][i][9]]) > 1 :
      if abs(data['year'][i] - year[data['VIN'][i][9]][0]) > abs(data['year'][i] - year[data['VIN'][i][9]][1]):
        data.loc[i,'year'] = year[data['VIN'][i][9]][1]
      else:
        data.loc[i,'year'] = year[data['VIN'][i][9]][0]
    else:
      data.loc[i,'year']= year[data['VIN'][i][9]]
    counter += 1
print('there are ',counter,' differences' )

"""filling blank years using VIN


"""

fig, ax = plt.subplots(figsize=(10,8))
ax.scatter( df['year'],df["price"])
plt.show()

"""# **Cleaning Price**"""

years = data['year'].unique()
counter = 0
for year in years:
  rslt_df = dc(data)
  yearly_data = data.loc[(data['year'] == year)]
  Q1 = np.quantile(yearly_data["price"],0.25)
  Q3 = np.quantile(yearly_data["price"],0.75)
  IQR = Q3 - Q1
  min = np.maximum((Q1 - 0.5 * IQR) , 500.0 + (year - 1980)*250)
  max = np.minimum((Q3 + 10 * IQR), 1700000.0)
  rslt_df = rslt_df[(rslt_df['price'] > min) & (rslt_df['price'] < max) & (rslt_df['year'] == year)]
  if counter == 0 : 
    final_df = dc(rslt_df)
  else:
    final_df = final_df.append(rslt_df, ignore_index=True, sort=False)
  counter += 1
final_df = final_df.reset_index(drop=True)
fig, ax = plt.subplots(figsize=(10,8))
ax.scatter( final_df['year'],final_df["price"])
plt.show()

"""# **Cope with Not available data**

Filling NA 'type' using mode of each model
"""

temp = dc(final_df)
temp.dropna(subset = ['model'],inplace=True)
temp = temp.reset_index(drop=True)
models = temp['model'].unique()
for model in models :
  if pd.isnull(temp.loc[(temp['model'] == model)]['type']).all():
    continue
  temp.loc[temp.model == model, "type"] = dc(pd.Series.mode(temp.loc[(temp['model'] == model)]['type'])[0])

final_df = dc(temp)
final_df.isna().sum()

"""**Filling NA 'cylinders' using mdoel**"""

temp = dc(final_df)
models = temp['model'].unique()
for model in models :
  if pd.isnull(temp.loc[(temp['model'] == model)]['cylinders']).all():
    continue
  temp.loc[temp.model == model, "cylinders"] = temp.loc[temp.model == model, "cylinders"].fillna(pd.Series.mode(temp.loc[(temp['model'] == model)]['cylinders'])[0])

final_df = dc(temp)
final_df.isna().sum()

"""**Filling NA 'transmission' using mdoel**"""

temp = dc(final_df)
models = temp['model'].unique()
for model in models :
  if pd.isnull(temp.loc[(temp['model'] == model)]['transmission']).all():
    continue
  temp.loc[temp.model == model, "transmission"] = temp.loc[temp.model == model, "transmission"].fillna(pd.Series.mode(temp.loc[(temp['model'] == model)]['transmission'])[0])

final_df = dc(temp)
final_df.isna().sum()

"""**Filling NA 'fuel' using mdoel**"""

temp = dc(final_df)
models = temp['model'].unique()
for model in models :
  if pd.isnull(temp.loc[(temp['model'] == model)]['fuel']).all():
    continue
  temp.loc[temp.model == model, "fuel"] = temp.loc[temp.model == model, "fuel"].fillna(pd.Series.mode(temp.loc[(temp['model'] == model)]['fuel'])[0])

final_df = dc(temp)
final_df.isna().sum()

"""**Filling NA 'paint_color' using mdoel**"""

temp = dc(final_df)
models = temp['model'].unique()
for model in models :
  if pd.isnull(temp.loc[(temp['model'] == model)]['paint_color']).all():
    continue
  temp.loc[temp.model == model, "paint_color"] = temp.loc[temp.model == model, "paint_color"].fillna(pd.Series.mode(temp.loc[(temp['model'] == model)]['paint_color'])[0])

final_df = dc(temp)
final_df.isna().sum()

"""**Filling NA 'manufacturer' using modell**"""

temp = dc(final_df)
models = temp['model'].unique()
for model in models :
  if pd.isnull(temp.loc[(temp['model'] == model)]['manufacturer']).all():
    continue
  temp.loc[temp.model == model, "manufacturer"] = temp.loc[temp.model == model, "manufacturer"].fillna(pd.Series.mode(temp.loc[(temp['model'] == model)]['manufacturer'])[0])

final_df = dc(temp)

"""**Filling NA 'odometer' using model and year**"""

temp = dc(final_df)

models = temp['model'].unique()
for model in models :
  years = temp.loc[(temp['model'] == models[0])]['year'].unique()
  for year in years :
    if pd.isnull(temp.loc[(temp['model'] == model) & (temp['year'] == year)]['odometer']).all():
      continue
    temp.loc[(temp['model'] == model) & (temp['year'] == year) , "odometer"] = temp.loc[(temp['model'] == model) & (temp['year'] == year), "odometer"].fillna(pd.Series.mean(temp.loc[(temp['model'] == model) & (temp['year'] == year)]['odometer'])[0])

safe_backup = dc(temp)

final_df = dc(temp)
final_df.isna().sum()

"""# **Removing Redundant Data**


"""

# final_df = final_df.drop('VIN', axis = 1)
# final_df= final_df.drop('condition', axis = 1)
final_df = final_df.dropna()
final_df.isna().sum()
final_df.count()

"""# **Removing Duplicated Data**


"""

final_df = final_df.drop_duplicates()

print(final_df.shape)
print(final_df.describe(include =[object]))
final_df.isna().sum()

"""# **Converting qualitative data into quantitative data**

**SENA**
"""

# final_df= final_df.drop('VIN', axis = 1)
# final_df = final_df.dropna()
# final_df = final_df.drop_duplicates()
final_df = final_df.reset_index(drop=True)
final_df.dtypes

final_df["region"].value_counts()
#print(final_df.shape)

!pip install pyspark
from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString
from pyspark.sql import SparkSession as spark
from pyspark import SparkContext
from pyspark.ml import Pipeline
sc = SparkContext()

spark= spark.builder.getOrCreate()
spark_one = spark.createDataFrame(final_df.astype(str))
spark_one.printSchema()
backup = dc(final_df)

"""**Converting Region, Manufecturer and State columns into numeric values according to frequency**"""

spark_one.createOrReplaceTempView("region")
one_df = spark_one.select("region")
one_df.show(5)

indexer = StringIndexer(inputCol="region", outputCol="region index")
indexed = indexer.fit(one_df).transform(one_df)

result_df = indexed.select("*").toPandas()
final_df['region index'] = result_df["region index"]
final_df.head()

spark_one.createOrReplaceTempView("state")
one_df = spark_one.select("state")
# one_df.show(5)

indexer = StringIndexer(inputCol="state", outputCol="state index")
indexed = indexer.fit(one_df).transform(one_df)
indexed.show(5)
result_df = indexed.select("*").toPandas()
final_df['state index'] = result_df["state index"]
final_df.head()

spark_one.createOrReplaceTempView("manufacturer")
one_df = spark_one.select("manufacturer")
# one_df.show(5)

indexer = StringIndexer(inputCol="manufacturer", outputCol="manufacturer index")
indexed = indexer.fit(one_df).transform(one_df)
indexed.show(5)
result_df = indexed.select("*").toPandas()
final_df['manufacturer index'] = result_df["manufacturer index"]
final_df.head()

spark_one.createOrReplaceTempView("type")
one_df = spark_one.select("type")

indexer = StringIndexer(inputCol="type", outputCol="type index")
indexed = indexer.fit(one_df).transform(one_df)

result_df = indexed.select("*").toPandas()
final_df['type index'] = result_df["type index"]
final_df.head()

spark_one.createOrReplaceTempView("model")
one_df = spark_one.select("model")

indexer = StringIndexer(inputCol="model", outputCol="model index")
indexed = indexer.fit(one_df).transform(one_df)

result_df = indexed.select("*").toPandas()
final_df['model index'] = result_df["model index"]
final_df.head()

# final_df.groupby('condition')['price'].mean().plot(kind='bar')

# final_df.groupby('condition')['price'].median().plot(kind='bar')

# condition_dict = {
#    "salvage": 1,
#    "fair": 0,
#    "like new": 3,
#    "excellent": 2,
#    "good": 4,
#    "new": 5
#  }
# final_df['condition point']= final_df['condition'].map(condition_dict)
final_df = final_df.drop(['manufacturer', 'state', 'region', 'type', 'model'], axis = 1) 

final_df = pd.get_dummies(final_df, columns = ['title_status', 'fuel', 'paint_color', 'transmission'])
final_df['cylinders'] = final_df['cylinders'].str.replace(' cylinders', '')
final_df

final_df

final_df.dtypes

"""# **Prediction**"""

final_data = dc(pd.get_dummies(final_df))

final_data

#Using Pearson Correlation
plt.figure(figsize=(18,12))
cor = final_data.corr()
sns.heatmap(cor, annot=False, cmap=plt.cm.Reds)
plt.show()

labels = np.array(final_data['price'])
final_data= final_data.drop('price', axis = 1)

"""# 1- OLS"""

train_x, test_x, train_y, test_y = train_test_split(final_data, labels, test_size = 0.20, shuffle=False)

regression = sm.OLS(train_y, train_x)
model = regression.fit()
testpred = model.predict(test_x)
testpred  = testpred.round(0)

print ("OLS Test Accuracy: ", metrics.accuracy_score(test_y, testpred))
print ("OLS Test MSE: ", metrics.mean_squared_error(test_y, testpred))
print(model.summary())

"""# 2- Logistic Regression"""

train_x, test_x, train_y, test_y = train_test_split(final_data, labels, test_size = 0.20, shuffle=False)

scaled_features = StandardScaler().fit_transform(train_x)
train_x = pd.DataFrame(scaled_features, index=train_x.index, columns=train_x.columns)
scaled_features = StandardScaler().fit_transform(test_x)
test_x = pd.DataFrame(scaled_features, index=test_x.index, columns=test_x.columns)
train_x = sm.add_constant(train_x)
test_x = sm.add_constant(test_x)
logreg = LogisticRegression(max_iter = 1000, solver = 'newton-cg').fit(train_x, train_y)

print ("LR Test Accuracy: ", metrics.accuracy_score(test_y, logreg.predict(test_x).round(0)))
print ("LR Test MSE: ", metrics.mean_squared_error(test_y, logreg.predict(test_x.round(0))))

"""# 3- Ridge Regression"""

train_x, test_x, train_y, test_y = train_test_split(final_data, labels, test_size = 0.20, shuffle=False)

scaled_features = StandardScaler().fit_transform(train_x)
train_x = pd.DataFrame(scaled_features, index=train_x.index, columns=train_x.columns)
scaled_features = StandardScaler().fit_transform(test_x)
test_x = pd.DataFrame(scaled_features, index=test_x.index, columns=test_x.columns)

train_x = sm.add_constant(train_x)
test_x = sm.add_constant(test_x)

RidReg=Lasso().fit(train_x, train_y)

print ("LR Test Accuracy: ", metrics.accuracy_score(test_y, RidReg.predict(test_x).round(0)))
print ("LR Test MSE: ", metrics.mean_squared_error(test_y, RidReg.predict(test_x).round(0)))

"""# 4- Random Forest Regression"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

train_x, test_x, train_y, test_y = train_test_split(final_data, labels, test_size = 0.20, random_state = 42)

sc = StandardScaler()
train_x = sc.fit_transform(train_x)
test_x = sc.transform(test_x)

regressor = RandomForestRegressor(n_estimators=20, random_state=0)
regressor.fit(train_x, train_y)
y_pred = regressor.predict(test_x)

print('Mean Absolute Error:', metrics.mean_absolute_error(test_y, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(test_y, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(test_y, y_pred)))

"""# Feature Engineering
## Selection Methods
### Backward Elimination
"""

def backward_elimination(data, target,significance_level):
    features = data.columns.tolist()
    while(len(features)>0):
        features_with_constant = sm.add_constant(data[features])
        p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]
        max_p_value = p_values.max()
        if(max_p_value >= significance_level):
            excluded_feature = p_values.idxmax()
            features.remove(excluded_feature)
        else:
            break 
    return features

best=backward_elimination(train_x, train_y,0.05)
best

"""# **4- OLS**"""

train_x, test_x, train_y, test_y = train_test_split(df, labels, test_size = 0.20, shuffle=False)

regression = sm.OLS(train_y, train_x)
model = regression.fit()
testpred = model.predict(test_x)
testpred  = testpred.round(0)

print ("OLS Test Accuracy: ", metrics.accuracy_score(test_y, testpred))
print ("OLS Test MSE: ", metrics.mean_squared_error(test_y, testpred))
print(model.summary())

"""

```
# This is formatted as code
```

"""

